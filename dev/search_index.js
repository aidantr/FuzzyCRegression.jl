var documenterSearchIndex = {"docs":
[{"location":"theory/#Fuzzy-C-Regression","page":"Fuzzy C-Regression","title":"Fuzzy C-Regression","text":"","category":"section"},{"location":"theory/#FCR-Objective","page":"Fuzzy C-Regression","title":"FCR Objective","text":"","category":"section"},{"location":"theory/","page":"Fuzzy C-Regression","title":"Fuzzy C-Regression","text":"Consider a linear model with grouped heterogeneity:","category":"page"},{"location":"theory/","page":"Fuzzy C-Regression","title":"Fuzzy C-Regression","text":"y_it = sum_g=1^Gmu_g(i) theta_gtX_it+varepsilon_it","category":"page"},{"location":"theory/","page":"Fuzzy C-Regression","title":"Fuzzy C-Regression","text":"where X_it are covariates (which could simply be a constant), theta_g represent group-specific coefficients for groups g=1ldotsG, and mu_g(i) represent group weights for unit i. Fuzzy Clustering Regression (FCR) is concerned with jointly estimating theta and mu for each group.","category":"page"},{"location":"theory/","page":"Fuzzy C-Regression","title":"Fuzzy C-Regression","text":"The FCR objective function takes the form:","category":"page"},{"location":"theory/","page":"Fuzzy C-Regression","title":"Fuzzy C-Regression","text":"L^FCR_mleft(thetamuright)=Eleftsum_g=1^Gmu_g^mleftVert y-theta_g xrightVert ^2right","category":"page"},{"location":"theory/","page":"Fuzzy C-Regression","title":"Fuzzy C-Regression","text":"where m  1 is the regularization parameter. The weights are defined as","category":"page"},{"location":"theory/","page":"Fuzzy C-Regression","title":"Fuzzy C-Regression","text":"mu_gleft(yxthetamright)=left(sum_h=1^GfracleftVert y-theta_g xrightVert ^2left(m-1right)leftVert y-theta_h xrightVert ^2left(m-1right)right)^-1 text for  g=1ldotsG","category":"page"},{"location":"theory/","page":"Fuzzy C-Regression","title":"Fuzzy C-Regression","text":"Combing these two equations, we can write the FCR objective as a continuous function of only the group-specific errors y-theta_gX:","category":"page"},{"location":"theory/","page":"Fuzzy C-Regression","title":"Fuzzy C-Regression","text":"J^FCRleft(thetaright)=Eleftleft(sum_g=1^GleftVert y-theta_g xrightVert ^-2left(m-1right)right)^1-mright","category":"page"},{"location":"theory/","page":"Fuzzy C-Regression","title":"Fuzzy C-Regression","text":"Thus, for fixed m, the FCR function is differentiable and can be written as a standard GMM problem. ","category":"page"},{"location":"theory/#Useful-Properties","page":"Fuzzy C-Regression","title":"Useful Properties","text":"","category":"section"},{"location":"theory/","page":"Fuzzy C-Regression","title":"Fuzzy C-Regression","text":"FCR can be solved in a single step through standard non-linear minimization. This makes it substantially faster than previous approaches, which require iteration over all possible groupings of units. While FCR remains quite efficient with large datasets, computation times for iterative algorithms become prohibitive.\nThe “fuzziness” of the FCR groups is governed by the regularization parameter m, where group assignment becomes binary as m rightarrow 1. Choosing m allows one to better accommodate the uncertainty of group membership in realistic datasets, where noise means that cluster membership cannot be ascertained with certainty. Moreover, it means that FCR can recover the full distribution of effects.\nSince FCR is a GMM problem, it's asymptotic properties follow from standard theory and we are able to derive analytic standard errors. ","category":"page"},{"location":"package/#FuzzyCRegression.jl-Manual","page":"Julia Implementation","title":"FuzzyCRegression.jl Manual","text":"","category":"section"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"Heterogeneous effects estimator from Lewis et al (2022) in Julia","category":"page"},{"location":"package/#Installation","page":"Julia Implementation","title":"Installation","text":"","category":"section"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"Pkg.add(\"FuzzyCRegression\")","category":"page"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"will install this package and its dependencies.","category":"page"},{"location":"package/#Fitting-the-FCR-model","page":"Julia Implementation","title":"Fitting the FCR model","text":"","category":"section"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"Two methods can be used to fit a Generalized Linear Model (GLM): glm(formula, data, family, link) and glm(X, y, family, link). Their arguments must be:","category":"page"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"formula: a StatsModels.jl Formula object referring to columns in data; for example, if column names are :Y, :X1, and :X2, then a valid formula is @formula(Y ~ X1 + X2)\ndata: a table in the Tables.jl definition, e.g. a data frame; rows with missing values are ignored\nX a matrix holding values of the independent variable(s) in columns\ny a vector holding values of the dependent variable (including if appropriate the intercept)\nfamily: chosen from Bernoulli(), Binomial(), Gamma(), Geometric(), Normal(), Poisson(), or NegativeBinomial(θ)\nlink: chosen from the list below, for example, LogitLink() is a valid link for the Binomial() family","category":"page"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"Typical distributions for use with glm and their canonical link functions are","category":"page"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"       Bernoulli (LogitLink)\n        Binomial (LogitLink)\n           Gamma (InverseLink)\n       Geometric (LogLink)\n InverseGaussian (InverseSquareLink)\nNegativeBinomial (NegativeBinomialLink, often used with LogLink)\n          Normal (IdentityLink)\n         Poisson (LogLink)","category":"page"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"Currently the available Link types are","category":"page"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"CauchitLink\nCloglogLink\nIdentityLink\nInverseLink\nInverseSquareLink\nLogitLink\nLogLink\nNegativeBinomialLink\nPowerLink\nProbitLink\nSqrtLink","category":"page"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"Note that the canonical link for negative binomial regression is NegativeBinomialLink, but in practice one typically uses LogLink. The NegativeBinomial distribution belongs to the exponential family only if θ (the shape parameter) is fixed, thus θ has to be provided if we use glm with NegativeBinomial family. If one would like to also estimate θ, then negbin(formula, data, link) should be used instead.","category":"page"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"An intercept is included in any GLM by default.","category":"page"},{"location":"package/#Categorical-variables","page":"Julia Implementation","title":"Categorical variables","text":"","category":"section"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"Categorical variables will be dummy coded by default if they are non-numeric or if they are CategoricalVectors within a Tables.jl table (DataFrame, JuliaDB table, named tuple of vectors, etc). Alternatively, you can pass an explicit contrasts argument if you would like a different contrast coding system or if you are not using DataFrames.","category":"page"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"The response (dependent) variable may not be categorical.","category":"page"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"Using a CategoricalVector constructed with categorical or categorical!:","category":"page"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"julia> using CategoricalArrays, DataFrames, GLM, StableRNGs\n\njulia> rng = StableRNG(1); # Ensure example can be reproduced\n\njulia> data = DataFrame(y = rand(rng, 100), x = categorical(repeat([1, 2, 3, 4], 25)));\n\n\njulia> lm(@formula(y ~ x), data)\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\ny ~ 1 + x\n\nCoefficients:\n───────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error      t  Pr(>|t|)   Lower 95%  Upper 95%\n───────────────────────────────────────────────────────────────────────────\n(Intercept)   0.490985    0.0564176   8.70    <1e-13   0.378997    0.602973\nx: 2          0.0527655   0.0797865   0.66    0.5100  -0.105609    0.21114\nx: 3          0.0955446   0.0797865   1.20    0.2341  -0.0628303   0.25392\nx: 4         -0.032673    0.0797865  -0.41    0.6831  -0.191048    0.125702\n───────────────────────────────────────────────────────────────────────────","category":"page"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"Using contrasts:","category":"page"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"julia> using StableRNGs\n\njulia> data = DataFrame(y = rand(StableRNG(1), 100), x = repeat([1, 2, 3, 4], 25));\n\njulia> lm(@formula(y ~ x), data, contrasts = Dict(:x => DummyCoding()))\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\ny ~ 1 + x\n\nCoefficients:\n───────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error      t  Pr(>|t|)   Lower 95%  Upper 95%\n───────────────────────────────────────────────────────────────────────────\n(Intercept)   0.490985    0.0564176   8.70    <1e-13   0.378997    0.602973\nx: 2          0.0527655   0.0797865   0.66    0.5100  -0.105609    0.21114\nx: 3          0.0955446   0.0797865   1.20    0.2341  -0.0628303   0.25392\nx: 4         -0.032673    0.0797865  -0.41    0.6831  -0.191048    0.125702\n───────────────────────────────────────────────────────────────────────────","category":"page"},{"location":"package/#Comparing-models-with-F-test","page":"Julia Implementation","title":"Comparing models with F-test","text":"","category":"section"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"Comparisons between two or more linear models can be performed using the ftest function, which computes an F-test between each pair of subsequent models and reports fit statistics:","category":"page"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"julia> using DataFrames, GLM, StableRNGs\n\njulia> data = DataFrame(y = (1:50).^2 .+ randn(StableRNG(1), 50), x = 1:50);\n\njulia> ols_lin = lm(@formula(y ~ x), data);\n\njulia> ols_sq = lm(@formula(y ~ x + x^2), data);\n\njulia> ftest(ols_lin.model, ols_sq.model)\nF-test: 2 models fitted on 50 observations\n─────────────────────────────────────────────────────────────────────────────────\n     DOF  ΔDOF           SSR           ΔSSR      R²     ΔR²            F*   p(>F)\n─────────────────────────────────────────────────────────────────────────────────\n[1]    3        1731979.2266                 0.9399\n[2]    4     1       40.7581  -1731938.4685  1.0000  0.0601  1997177.0357  <1e-99\n─────────────────────────────────────────────────────────────────────────────────","category":"page"},{"location":"package/#Methods-applied-to-fitted-models","page":"Julia Implementation","title":"Methods applied to fitted models","text":"","category":"section"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"Many of the methods provided by this package have names similar to those in R.","category":"page"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"adjr2: adjusted R² for a linear model (an alias for adjr²)\naic: Akaike's Information Criterion\naicc: corrected Akaike's Information Criterion for small sample sizes\nbic: Bayesian Information Criterion\ncoef: estimates of the coefficients in the model\nconfint: confidence intervals for coefficients\ncooksdistance: Cook's distance for each observation\ndeviance: measure of the model fit, weighted residual sum of squares for lm's\ndispersion: dispersion (or scale) parameter for a model's distribution\ndof: number of degrees of freedom consumed in the model\ndof_residual: degrees of freedom for residuals, when meaningful\nfitted: fitted values of the model\nglm: fit a generalized linear model (an alias for fit(GeneralizedLinearModel, ...))\nlm: fit a linear model (an alias for fit(LinearModel, ...))\nloglikelihood: log-likelihood of the model\nmodelmatrix: design matrix\nnobs: number of rows, or sum of the weights when prior weights are specified\nnulldeviance: deviance of the model with all predictors removed\nnullloglikelihood: log-likelihood of the model with all predictors removed\npredict: predicted values of the dependent variable from the fitted model\nr2: R² of a linear model (an alias for r²)\nresiduals: vector of residuals from the fitted model\nresponse: model response (a.k.a the dependent variable)\nstderror: standard errors of the coefficients\nvcov: variance-covariance matrix of the coefficient estimates","category":"page"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"Note that the canonical link for negative binomial regression is NegativeBinomialLink, but in practice one typically uses LogLink.","category":"page"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"julia> using GLM, DataFrames, StatsBase\n\njulia> data = DataFrame(X=[1,2,3], y=[2,4,7]);\n\njulia> mdl = lm(@formula(y ~ X), data);\n\njulia> round.(coef(mdl); digits=8)\n2-element Vector{Float64}:\n -0.66666667\n  2.5\n\njulia> round(r2(mdl); digits=8)\n0.98684211\n\njulia> round(aic(mdl); digits=8)\n5.84251593","category":"page"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"The predict method returns predicted values of response variable from covariate values in an input newX. If newX is omitted then the fitted response values from the model are returned.","category":"page"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"julia> test_data = DataFrame(X=[4]);\n\njulia> round.(predict(mdl, test_data); digits=8)\n1-element Vector{Float64}:\n 9.33333333","category":"page"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"The cooksdistance method computes Cook's distance for each observation used to fit a linear model, giving an estimate of the influence of each data point. Note that it's currently only implemented for linear models without weights.","category":"page"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"julia> round.(cooksdistance(mdl); digits=8)\n3-element Vector{Float64}:\n 2.5\n 0.25\n 2.5","category":"page"},{"location":"package/#Separation-of-response-object-and-predictor-object","page":"Julia Implementation","title":"Separation of response object and predictor object","text":"","category":"section"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"The general approach in this code is to separate functionality related to the response from that related to the linear predictor.  This allows for greater generality by mixing and matching different subtypes of the abstract type LinPred and the abstract type ModResp.","category":"page"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"A LinPred type incorporates the parameter vector and the model matrix.  The parameter vector is a dense numeric vector but the model matrix can be dense or sparse.  A LinPred type must incorporate some form of a decomposition of the weighted model matrix that allows for the solution of a system X'W * X * delta=X'wres where W is a diagonal matrix of \"X weights\", provided as a vector of the square roots of the diagonal elements, and wres is a weighted residual vector.","category":"page"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"Currently there are two dense predictor types, DensePredQR and DensePredChol, and the usual caveats apply.  The Cholesky version is faster but somewhat less accurate than that QR version. The skeleton of a distributed predictor type is in the code but not yet fully fleshed out.  Because Julia by default uses OpenBLAS, which is already multi-threaded on multicore machines, there may not be much advantage in using distributed predictor types.","category":"page"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"A ModResp type must provide methods for the wtres and sqrtxwts generics.  Their values are the arguments to the updatebeta methods of the LinPred types.  The Float64 value returned by updatedelta is the value of the convergence criterion.","category":"page"},{"location":"package/","page":"Julia Implementation","title":"Julia Implementation","text":"Similarly, LinPred types must provide a method for the linpred generic.  In general linpred takes an instance of a LinPred type and a step factor.  Methods that take only an instance of a LinPred type use a default step factor of 1.  The value of linpred is the argument to the updatemu method for ModResp types.  The updatemu method returns the updated deviance.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = FuzzyCRegression","category":"page"},{"location":"#FuzzyCRegression","page":"Home","title":"FuzzyCRegression","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for FuzzyCRegression.jl, which implements the heterogeneous effects estimator from Lewis, Melcangi, Pilossoph, and Toner-Rodgers (2022)","category":"page"},{"location":"#Contents","page":"Home","title":"Contents","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"theory.md\", \"package.md\", \"API.md\"]","category":"page"},{"location":"API/","page":"API","title":"API","text":"Modules = [FuzzyCRegression]","category":"page"},{"location":"API/#FuzzyCRegression.distribution-Tuple{FuzzyCRegression.Model}","page":"API","title":"FuzzyCRegression.distribution","text":"distribution()\n\nCalculates distribution of weighted coefficients for fcr model output\n\nArguments\n\nresults::Model Model type from fcr output\nindex::Integer Column index of variable in X matrix to calculate coefficient distibution for (defaults to 1)\nplot:Bool Plot histogram of coefficients\n\n\n\n\n\n","category":"method"},{"location":"API/#FuzzyCRegression.fit-Tuple{}","page":"API","title":"FuzzyCRegression.fit","text":"fit()\n\nImplements fuzzy clustering regression estimator from Lewis, Melcangi, Pilossoph, and Toner-Rodgers (2022)\n\nArguments\n\ny::Vector dependent var\nx::Matrix variables with heterogeneous coefficients (defaults to vector of 1's for FEs)\nZ:matrix matrix of controls\nunit:Vector vector of unit IDs\nt::Vector time vector (optional)\nG::Integer number of groups (default = 2)\nm::Real fuzzy tuning parameter\nstartvals::Integer number of starting values (default = 1,000)\ncores::Integer number of cores (default = 1)\n\nReturns:\n\nstruct with the following methods:\n\ncoefficients::Vector: vector of coefficients\nweights::Matrix: group weights\nSE::Vector: standard errors (optional)\nobjective::Number value of objective function at minimum\n\n\n\n\n\n","category":"method"},{"location":"API/#FuzzyCRegression.information-Tuple{FuzzyCRegression.Model}","page":"API","title":"FuzzyCRegression.information","text":"information()\n\nCalculates inforation criteria for fcr model output, used for selecting optimal number of groups\n\nArguments\n\nresults::Model Model type from fcr output\ncriterion::String Information criterion (AIC, BIC, or HQC)\n\n\n\n\n\n","category":"method"}]
}
